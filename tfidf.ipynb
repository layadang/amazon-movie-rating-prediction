{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run `feature_extraction.py` and `train_test_split.ipynb` first!\n",
    "(in respective order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating TF-IDF Matrix\n",
    "And combining with X_train, X_test, and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"data/components/X_train.csv\")\n",
    "y_train = pd.read_csv(\"data/components/y_train.csv\")\n",
    "X_test = pd.read_csv(\"data/components/X_test.csv\")\n",
    "y_test = pd.read_csv(\"data/components/y_test.csv\")\n",
    "submission = pd.read_csv(\"data/components/submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "      <th>Helpful</th>\n",
       "      <th>Unhelpful</th>\n",
       "      <th>ExclaimationCount</th>\n",
       "      <th>...</th>\n",
       "      <th>TextCleaned</th>\n",
       "      <th>UniqueWords</th>\n",
       "      <th>LemmatizedSummary</th>\n",
       "      <th>LemmatizedCleanedText</th>\n",
       "      <th>SummarySentiment</th>\n",
       "      <th>CleanedTextSentiment</th>\n",
       "      <th>ProductAvgScore</th>\n",
       "      <th>UserAvgScore</th>\n",
       "      <th>ProductPopularity</th>\n",
       "      <th>UserPopularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B001VPJYZK</td>\n",
       "      <td>A1XKXV4BSW11TF</td>\n",
       "      <td>for my wife</td>\n",
       "      <td>Thank you for having this item, I live on Guam...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>thank item live guam find item wife checked am...</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>for my wife</td>\n",
       "      <td>thank item live guam find item wife checked am...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.7269</td>\n",
       "      <td>3.448276</td>\n",
       "      <td>5.0</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B000MTFDDI</td>\n",
       "      <td>A1374RLDD5VINW</td>\n",
       "      <td>no pink panther</td>\n",
       "      <td>there isn't one pink panther episode on the wh...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>pink panther episode whole dvd buy pink panthe...</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>no pink panther</td>\n",
       "      <td>pink panther episode whole dvd buy pink panthe...</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.909091</td>\n",
       "      <td>4.2</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ProductId          UserId          Summary  \\\n",
       "0  B001VPJYZK  A1XKXV4BSW11TF      for my wife   \n",
       "1  B000MTFDDI  A1374RLDD5VINW  no pink panther   \n",
       "\n",
       "                                                Text  Score  Month  Year  \\\n",
       "0  Thank you for having this item, I live on Guam...    5.0      8  2010   \n",
       "1  there isn't one pink panther episode on the wh...    1.0      1  2013   \n",
       "\n",
       "   Helpful  Unhelpful  ExclaimationCount  ...  \\\n",
       "0        0          0                  1  ...   \n",
       "1        0          5                  1  ...   \n",
       "\n",
       "                                         TextCleaned  UniqueWords  \\\n",
       "0  thank item live guam find item wife checked am...     0.857143   \n",
       "1  pink panther episode whole dvd buy pink panthe...     0.760000   \n",
       "\n",
       "   LemmatizedSummary                              LemmatizedCleanedText  \\\n",
       "0        for my wife  thank item live guam find item wife checked am...   \n",
       "1    no pink panther  pink panther episode whole dvd buy pink panthe...   \n",
       "\n",
       "   SummarySentiment CleanedTextSentiment ProductAvgScore  UserAvgScore  \\\n",
       "0             0.000               0.7269        3.448276           5.0   \n",
       "1            -0.296               0.0000        4.909091           4.2   \n",
       "\n",
       "   ProductPopularity  UserPopularity  \n",
       "0                 29               4  \n",
       "1                 11              10  \n",
       "\n",
       "[2 rows x 23 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure there are no NaNs\n",
    "X_train['Summary'] = X_train['Summary'].fillna(\"\")\n",
    "X_train['LemmatizedSummary'] = X_train['LemmatizedSummary'].fillna(\"\")\n",
    "X_test['Summary'] = X_test['Summary'].fillna(\"\")\n",
    "X_test['LemmatizedSummary'] = X_test['LemmatizedSummary'].fillna(\"\")\n",
    "submission['Summary'] = submission['Summary'].fillna(\"\")\n",
    "submission['LemmatizedSummary'] = submission['LemmatizedSummary'].fillna(\"\")\n",
    "\n",
    "X_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't include words frequent as 55%+ and as infrequent as 1% of text\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df = 0.55, min_df = 0.01, ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_text_train = X_train['TextCleaned'] + \" \" + X_train['Summary']\n",
    "X_train_tfidf_full = tfidf_vectorizer.fit_transform(combined_text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_text_test = X_test['TextCleaned'] + \" \" + X_test['Summary']\n",
    "X_test_tfidf_full = tfidf_vectorizer.transform(combined_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_text_submission = submission['TextCleaned'] + \" \" + submission['Summary']\n",
    "submission_tfidf_full = tfidf_vectorizer.transform(combined_text_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1188268, 1262)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_tfidf(X, tfidf):\n",
    "    X_tfidf = pd.DataFrame(tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "    X_tfidf.reset_index(drop=True, inplace=True)\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    X_combined_df = pd.concat([X_tfidf, X], axis=1)\n",
    "\n",
    "    return X_combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['ProductId', 'UserId', 'Summary', 'Text', 'LemmatizedSummary', 'LemmatizedCleanedText', 'TextCleaned','Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>across</th>\n",
       "      <th>act</th>\n",
       "      <th>acted</th>\n",
       "      <th>acting</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actors</th>\n",
       "      <th>...</th>\n",
       "      <th>TextCleaned</th>\n",
       "      <th>UniqueWords</th>\n",
       "      <th>LemmatizedSummary</th>\n",
       "      <th>LemmatizedCleanedText</th>\n",
       "      <th>SummarySentiment</th>\n",
       "      <th>CleanedTextSentiment</th>\n",
       "      <th>ProductAvgScore</th>\n",
       "      <th>UserAvgScore</th>\n",
       "      <th>ProductPopularity</th>\n",
       "      <th>UserPopularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>believe parents problem scary gore nailbiter s...</td>\n",
       "      <td>0.670103</td>\n",
       "      <td>a horror classic</td>\n",
       "      <td>believe parent problem scary gore nailbiter sa...</td>\n",
       "      <td>-0.5719</td>\n",
       "      <td>-0.8781</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.781513</td>\n",
       "      <td>50</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>blind fury good simple could omega doom promis...</td>\n",
       "      <td>0.851064</td>\n",
       "      <td>blind fury is a good movie but omega doom left...</td>\n",
       "      <td>blind fury good simple could omega doom promis...</td>\n",
       "      <td>-0.8151</td>\n",
       "      <td>-0.8834</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.916667</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 1285 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ability  able  absolutely  across  act  acted  acting  action  actor  \\\n",
       "0      0.0   0.0         0.0     0.0  0.0    0.0     0.0     0.0    0.0   \n",
       "1      0.0   0.0         0.0     0.0  0.0    0.0     0.0     0.0    0.0   \n",
       "\n",
       "   actors  ...                                        TextCleaned  \\\n",
       "0     0.0  ...  believe parents problem scary gore nailbiter s...   \n",
       "1     0.0  ...  blind fury good simple could omega doom promis...   \n",
       "\n",
       "   UniqueWords                                  LemmatizedSummary  \\\n",
       "0     0.670103                                   a horror classic   \n",
       "1     0.851064  blind fury is a good movie but omega doom left...   \n",
       "\n",
       "                               LemmatizedCleanedText  SummarySentiment  \\\n",
       "0  believe parent problem scary gore nailbiter sa...           -0.5719   \n",
       "1  blind fury good simple could omega doom promis...           -0.8151   \n",
       "\n",
       "   CleanedTextSentiment  ProductAvgScore  UserAvgScore  ProductPopularity  \\\n",
       "0               -0.8781              3.2      3.781513                 50   \n",
       "1               -0.8834              2.5      2.916667                  8   \n",
       "\n",
       "   UserPopularity  \n",
       "0             119  \n",
       "1              12  \n",
       "\n",
       "[2 rows x 1285 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_combined = combine_tfidf(X_train, X_train_tfidf_full)\n",
    "X_train_combined.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>across</th>\n",
       "      <th>act</th>\n",
       "      <th>acted</th>\n",
       "      <th>acting</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actors</th>\n",
       "      <th>...</th>\n",
       "      <th>TextCleaned</th>\n",
       "      <th>UniqueWords</th>\n",
       "      <th>LemmatizedSummary</th>\n",
       "      <th>LemmatizedCleanedText</th>\n",
       "      <th>SummarySentiment</th>\n",
       "      <th>CleanedTextSentiment</th>\n",
       "      <th>ProductAvgScore</th>\n",
       "      <th>UserAvgScore</th>\n",
       "      <th>ProductPopularity</th>\n",
       "      <th>UserPopularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>thank item live guam find item wife checked am...</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>for my wife</td>\n",
       "      <td>thank item live guam find item wife checked am...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.7269</td>\n",
       "      <td>3.448276</td>\n",
       "      <td>5.0</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>pink panther episode whole dvd buy pink panthe...</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>no pink panther</td>\n",
       "      <td>pink panther episode whole dvd buy pink panthe...</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.909091</td>\n",
       "      <td>4.2</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 1285 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ability  able  absolutely  across  act  acted  acting  action  actor  \\\n",
       "0      0.0   0.0         0.0     0.0  0.0    0.0     0.0     0.0    0.0   \n",
       "1      0.0   0.0         0.0     0.0  0.0    0.0     0.0     0.0    0.0   \n",
       "\n",
       "   actors  ...                                        TextCleaned  \\\n",
       "0     0.0  ...  thank item live guam find item wife checked am...   \n",
       "1     0.0  ...  pink panther episode whole dvd buy pink panthe...   \n",
       "\n",
       "   UniqueWords  LemmatizedSummary  \\\n",
       "0     0.857143        for my wife   \n",
       "1     0.760000    no pink panther   \n",
       "\n",
       "                               LemmatizedCleanedText  SummarySentiment  \\\n",
       "0  thank item live guam find item wife checked am...             0.000   \n",
       "1  pink panther episode whole dvd buy pink panthe...            -0.296   \n",
       "\n",
       "   CleanedTextSentiment  ProductAvgScore  UserAvgScore  ProductPopularity  \\\n",
       "0                0.7269         3.448276           5.0                 29   \n",
       "1                0.0000         4.909091           4.2                 11   \n",
       "\n",
       "   UserPopularity  \n",
       "0               4  \n",
       "1              10  \n",
       "\n",
       "[2 rows x 1285 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_combined = combine_tfidf(X_test, X_test_tfidf_full)\n",
    "X_test_combined.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>across</th>\n",
       "      <th>act</th>\n",
       "      <th>acted</th>\n",
       "      <th>acting</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actors</th>\n",
       "      <th>...</th>\n",
       "      <th>TextCleaned</th>\n",
       "      <th>UniqueWords</th>\n",
       "      <th>LemmatizedSummary</th>\n",
       "      <th>LemmatizedCleanedText</th>\n",
       "      <th>SummarySentiment</th>\n",
       "      <th>CleanedTextSentiment</th>\n",
       "      <th>ProductAvgScore</th>\n",
       "      <th>UserAvgScore</th>\n",
       "      <th>ProductPopularity</th>\n",
       "      <th>UserPopularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.226498</td>\n",
       "      <td>...</td>\n",
       "      <td>alright people saying ensemble cast misleading...</td>\n",
       "      <td>0.650888</td>\n",
       "      <td>okay for a rental</td>\n",
       "      <td>alright people saying ensemble cast misleading...</td>\n",
       "      <td>0.2263</td>\n",
       "      <td>0.9866</td>\n",
       "      <td>3.478548</td>\n",
       "      <td>2.545455</td>\n",
       "      <td>303</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>kids love exciting fun watch well written kids...</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>great for kid</td>\n",
       "      <td>kid love exciting fun watch well written kid w...</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>0.9153</td>\n",
       "      <td>4.182927</td>\n",
       "      <td>4.214286</td>\n",
       "      <td>82</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 1287 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ability  able  absolutely  across  act  acted  acting  action  actor  \\\n",
       "0      0.0   0.0         0.0     0.0  0.0    0.0     0.0     0.0    0.0   \n",
       "1      0.0   0.0         0.0     0.0  0.0    0.0     0.0     0.0    0.0   \n",
       "\n",
       "     actors  ...                                        TextCleaned  \\\n",
       "0  0.226498  ...  alright people saying ensemble cast misleading...   \n",
       "1  0.000000  ...  kids love exciting fun watch well written kids...   \n",
       "\n",
       "   UniqueWords  LemmatizedSummary  \\\n",
       "0     0.650888  okay for a rental   \n",
       "1     0.950000      great for kid   \n",
       "\n",
       "                               LemmatizedCleanedText  SummarySentiment  \\\n",
       "0  alright people saying ensemble cast misleading...            0.2263   \n",
       "1  kid love exciting fun watch well written kid w...            0.6249   \n",
       "\n",
       "   CleanedTextSentiment  ProductAvgScore  UserAvgScore  ProductPopularity  \\\n",
       "0                0.9866         3.478548      2.545455                303   \n",
       "1                0.9153         4.182927      4.214286                 82   \n",
       "\n",
       "   UserPopularity  \n",
       "0              22  \n",
       "1              14  \n",
       "\n",
       "[2 rows x 1287 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_combined = combine_tfidf(submission, submission_tfidf_full)\n",
    "submission_combined.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combined = X_train_combined.drop(columns=columns_to_drop, axis=1)\n",
    "X_test_combined = X_test_combined.drop(columns=columns_to_drop, axis=1)\n",
    "submission_combined = submission_combined.drop(columns=columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((212192, 1287), (297067, 1285), (1188268, 1285))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_combined.shape, X_test_combined.shape, X_train_combined.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combined.to_csv(\"data/components/tfidf/X_train_full.csv\", index=False, header=True)\n",
    "X_test_combined.to_csv(\"data/components/tfidf/X_test_full.csv\", index=False, header=True)\n",
    "submission_combined.to_csv(\"data/components/tfidf/submission_full.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe Experiments\n",
    "(this didn't work out, so ignore this section please)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "glove_embeddings = load_glove_embeddings('experiments/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vector(words, glove_embeddings):\n",
    "    vectors = [glove_embeddings.get(word) for word in words if word in glove_embeddings]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1188268, 15)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['SentenceVector'] = X_train['LemmatizedCleanedText'].apply(lambda x: get_sentence_vector(x, glove_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['SentenceVector'] = X_test['LemmatizedCleanedText'].apply(lambda x: get_sentence_vector(x, glove_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SummarySentiment</th>\n",
       "      <th>CleanedTextSentiment</th>\n",
       "      <th>ProductAvgScore</th>\n",
       "      <th>UserAvgScore</th>\n",
       "      <th>SentenceVector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.5719</td>\n",
       "      <td>-0.8781</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>3.781513</td>\n",
       "      <td>[-0.2892621, 0.075449675, -0.28249758, -0.0367...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.8151</td>\n",
       "      <td>-0.8834</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.916667</td>\n",
       "      <td>[-0.25809747, 0.07172786, -0.29970953, -0.0479...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6808</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>1.772727</td>\n",
       "      <td>[-0.26813552, 0.016843056, -0.3056523, -0.0527...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.8271</td>\n",
       "      <td>0.9081</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>3.888889</td>\n",
       "      <td>[-0.31486198, 0.0812924, -0.2991736, -0.060852...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.2732</td>\n",
       "      <td>-0.1027</td>\n",
       "      <td>3.690909</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>[-0.26775372, 0.08156801, -0.3879338, -0.07389...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499995</th>\n",
       "      <td>-0.1779</td>\n",
       "      <td>0.8271</td>\n",
       "      <td>3.416667</td>\n",
       "      <td>4.833333</td>\n",
       "      <td>[-0.27551064, 0.040809095, -0.32630408, -0.052...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499996</th>\n",
       "      <td>0.6369</td>\n",
       "      <td>0.9578</td>\n",
       "      <td>4.175115</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>[-0.1864752, 0.03579887, -0.32041645, -0.11680...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499997</th>\n",
       "      <td>0.7003</td>\n",
       "      <td>-0.7430</td>\n",
       "      <td>3.414239</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>[-0.24792093, 0.035204932, -0.27055123, -0.069...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499998</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9432</td>\n",
       "      <td>4.153846</td>\n",
       "      <td>3.769231</td>\n",
       "      <td>[-0.27007738, 0.034698863, -0.27886388, -0.063...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499999</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2263</td>\n",
       "      <td>3.214286</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>[-0.27543372, -0.002893933, -0.29457453, -0.06...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        SummarySentiment  CleanedTextSentiment  ProductAvgScore  UserAvgScore  \\\n",
       "0                -0.5719               -0.8781         3.200000      3.781513   \n",
       "1                -0.8151               -0.8834         2.500000      2.916667   \n",
       "2                 0.0000                0.6808         3.750000      1.772727   \n",
       "3                 0.8271                0.9081         3.750000      3.888889   \n",
       "4                 0.2732               -0.1027         3.690909      2.666667   \n",
       "...                  ...                   ...              ...           ...   \n",
       "499995           -0.1779                0.8271         3.416667      4.833333   \n",
       "499996            0.6369                0.9578         4.175115      3.500000   \n",
       "499997            0.7003               -0.7430         3.414239      3.200000   \n",
       "499998            0.0000                0.9432         4.153846      3.769231   \n",
       "499999            0.0000                0.2263         3.214286      1.125000   \n",
       "\n",
       "                                           SentenceVector  \n",
       "0       [-0.2892621, 0.075449675, -0.28249758, -0.0367...  \n",
       "1       [-0.25809747, 0.07172786, -0.29970953, -0.0479...  \n",
       "2       [-0.26813552, 0.016843056, -0.3056523, -0.0527...  \n",
       "3       [-0.31486198, 0.0812924, -0.2991736, -0.060852...  \n",
       "4       [-0.26775372, 0.08156801, -0.3879338, -0.07389...  \n",
       "...                                                   ...  \n",
       "499995  [-0.27551064, 0.040809095, -0.32630408, -0.052...  \n",
       "499996  [-0.1864752, 0.03579887, -0.32041645, -0.11680...  \n",
       "499997  [-0.24792093, 0.035204932, -0.27055123, -0.069...  \n",
       "499998  [-0.27007738, 0.034698863, -0.27886388, -0.063...  \n",
       "499999  [-0.27543372, -0.002893933, -0.29457453, -0.06...  \n",
       "\n",
       "[500000 rows x 5 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:500_000][['SummarySentiment', 'CleanedTextSentiment', 'ProductAvgScore', 'UserAvgScore', 'SentenceVector']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
